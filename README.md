# Support Agent Skeleton

This repository contains a minimal Node.js application that implements a single endpoint for a retrieval‑augmented generation (RAG) support agent. It is intended as a starting point for the live coding exercise in the lead backend developer interview.

## Prerequisites

- Node.js >= 14
- pnpm >= 8 (install with `npm install -g pnpm`)

## Installation

Install dependencies:

```sh
pnpm install
```

## Running the Server

### Production
```sh
pnpm build
pnpm start
```

### Development
```sh
pnpm dev
```

The server will listen on port 3000 by default. You can override the port by setting the `PORT` environment variable.

## Endpoint

`POST /support/chat`

Request body:

```json
{
  "question": "How do I reset my password?"
}
```

Response body:

```json
{
  "answer": "This is a placeholder answer generated by the mock LLM.",
  "sources": [
    { "id": "doc1", "score": 0.9 },
    { "id": "doc2", "score": 0.8 }
  ]
}
```

## Implementation notes

- All helper functions (`generateEmbedding`, `searchDocs`, `buildPrompt` and `callLLM`) are asynchronous or return promises. In a production system these would call external services (e.g. an embedding API, a vector database, and an LLM provider).
- Avoid blocking the Node.js event loop. If you need to perform CPU‑intensive work (e.g. computing embeddings locally), offload it to a worker thread or another service.
- Error handling is included in the route handler. In a real system you might want to add more granular error responses and logging.
- Feel free to expand this skeleton with features such as caching, streaming responses via Server‑Sent Events (SSE), rate limiting, or conversation memory.
